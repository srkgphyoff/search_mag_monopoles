{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69150704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4469cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating output file, factory object and opening input files\n",
    "outputFile = ROOT.TFile(\"TMVAMLP.root\", \"RECREATE\")\n",
    "factory = TMVA.Factory(\"tmvaTest\", outputFile, \"\")\n",
    "dataLoader = TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "trainFile = ROOT.TFile(\"smalltrainData.root\")\n",
    "testFile = ROOT.TFile(\"smalltestData.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baeba61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the TTree objects from input files\n",
    "sigTrain = trainFile.Get(\"sig\")\n",
    "bkgTrain = trainFile.Get(\"bkg\")\n",
    "nSigTrain = sigTrain.GetEntries()\n",
    "nBkgTrain = bkgTrain.GetEntries()\n",
    "\n",
    "sigTest = testFile.Get(\"sig\")\n",
    "bkgTest = testFile.Get(\"bkg\")\n",
    "nSigTest = sigTest.GetEntries()\n",
    "nBkgTest = bkgTest.GetEntries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9732535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104918, 1565055\n",
      "105370, 1563066\n",
      "1.0, 0.0670378996265307\n"
     ]
    }
   ],
   "source": [
    "#Print num events\n",
    "print(f\"{nSigTrain}, {nBkgTrain}\")\n",
    "print(f\"{nSigTest}, {nBkgTest}\")\n",
    "\n",
    "#GlobalEventWeights\n",
    "#Thus for equivalent weights\n",
    "sigWeight = 1.0\n",
    "bkgWeight = float(nSigTrain)/float(nBkgTrain)\n",
    "print(f\"{sigWeight}, {bkgWeight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a6235c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HEADER> DataSetInfo              : [dataset] : Added class \"Signal\"\n",
      "                         : Add Tree sig of type Signal with 104918 events\n",
      "<HEADER> DataSetInfo              : [dataset] : Added class \"Background\"\n",
      "                         : Add Tree bkg of type Background with 1565055 events\n",
      "                         : Add Tree sig of type Signal with 105370 events\n",
      "                         : Add Tree bkg of type Background with 1563066 events\n"
     ]
    }
   ],
   "source": [
    "dataLoader.AddSignalTree(sigTrain, sigWeight, TMVA.Types.kTraining)\n",
    "dataLoader.AddBackgroundTree(bkgTrain, bkgWeight, TMVA.Types.kTraining)\n",
    "dataLoader.AddSignalTree(sigTest, sigWeight, TMVA.Types.kTesting)\n",
    "dataLoader.AddBackgroundTree(bkgTest, bkgWeight, TMVA.Types.kTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c2b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the input variables that shall be used for the MVA training\n",
    "#(the variables used in the expression must exist in the original TTree).\n",
    "dataLoader.AddVariable(\"ADC_mean\", 'F')\n",
    "dataLoader.AddVariable(\"nhits_min\", 'F')\n",
    "dataLoader.AddVariable(\"entry_dist\", 'F')\n",
    "dataLoader.AddVariable(\"exit_dist\", 'F')\n",
    "dataLoader.AddVariable(\"docasqrx_max\", 'F')\n",
    "dataLoader.AddVariable(\"docasqry_max\", 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9024ca77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TMVA.MethodMLP object at 0x5ceb58b6c080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HEADER> Factory                  : Booking method: MLPfinal\n",
      "                         : \n",
      "<VERBOSE>                          : Parsing option string: \n",
      "<VERBOSE>                          : ... \"H:V:VarTransform=Norm:NCycles=300:HiddenLayers=N+10:TestRate=10:TrainingMethod=BP\"\n",
      "<VERBOSE>                          : The following options are set:\n",
      "<VERBOSE>                          : - By User:\n",
      "<VERBOSE>                          :     <none>\n",
      "<VERBOSE>                          : - Default:\n",
      "<VERBOSE>                          :     Boost_num: \"0\" [Number of times the classifier will be boosted]\n",
      "<VERBOSE>                          : Parsing option string: \n",
      "<VERBOSE>                          : ... \"H:V:VarTransform=Norm:NCycles=300:HiddenLayers=N+10:TestRate=10:TrainingMethod=BP\"\n",
      "<VERBOSE>                          : The following options are set:\n",
      "<VERBOSE>                          : - By User:\n",
      "<VERBOSE>                          :     NCycles: \"300\" [Number of training cycles]\n",
      "<VERBOSE>                          :     HiddenLayers: \"N+10\" [Specification of hidden layer architecture]\n",
      "<VERBOSE>                          :     V: \"True\" [Verbose output (short form of \"VerbosityLevel\" below - overrides the latter one)]\n",
      "<VERBOSE>                          :     VarTransform: \"Norm\" [List of variable transformations performed before training, e.g., \"D_Background,P_Signal,G,N_AllClasses\" for: \"Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)\"]\n",
      "<VERBOSE>                          :     H: \"True\" [Print method-specific help message]\n",
      "<VERBOSE>                          :     TrainingMethod: \"BP\" [Train with Back-Propagation (BP), BFGS Algorithm (BFGS), or Genetic Algorithm (GA - slower and worse)]\n",
      "<VERBOSE>                          :     TestRate: \"10\" [Test for overtraining performed at each #th epochs]\n",
      "<VERBOSE>                          : - Default:\n",
      "<VERBOSE>                          :     NeuronType: \"sigmoid\" [Neuron activation function type]\n",
      "<VERBOSE>                          :     RandomSeed: \"1\" [Random seed for initial synapse weights (0 means unique seed for each run; default value '1')]\n",
      "<VERBOSE>                          :     EstimatorType: \"MSE\" [MSE (Mean Square Estimator) for Gaussian Likelihood or CE(Cross-Entropy) for Bernoulli Likelihood]\n",
      "<VERBOSE>                          :     NeuronInputType: \"sum\" [Neuron input function type]\n",
      "<VERBOSE>                          :     VerbosityLevel: \"Default\" [Verbosity level]\n",
      "<VERBOSE>                          :     CreateMVAPdfs: \"False\" [Create PDFs for classifier outputs (signal and background)]\n",
      "<VERBOSE>                          :     IgnoreNegWeightsInTraining: \"False\" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]\n",
      "<VERBOSE>                          :     LearningRate: \"2.000000e-02\" [ANN learning rate parameter]\n",
      "<VERBOSE>                          :     DecayRate: \"1.000000e-02\" [Decay rate for learning parameter]\n",
      "<VERBOSE>                          :     EpochMonitoring: \"False\" [Provide epoch-wise monitoring plots according to TestRate (caution: causes big ROOT output file!)]\n",
      "<VERBOSE>                          :     Sampling: \"1.000000e+00\" [Only 'Sampling' (randomly selected) events are trained each epoch]\n",
      "<VERBOSE>                          :     SamplingEpoch: \"1.000000e+00\" [Sampling is used for the first 'SamplingEpoch' epochs, afterwards, all events are taken for training]\n",
      "<VERBOSE>                          :     SamplingImportance: \"1.000000e+00\" [ The sampling weights of events in epochs which successful (worse estimator than before) are multiplied with SamplingImportance, else they are divided.]\n",
      "<VERBOSE>                          :     SamplingTraining: \"True\" [The training sample is sampled]\n",
      "<VERBOSE>                          :     SamplingTesting: \"False\" [The testing sample is sampled]\n",
      "<VERBOSE>                          :     ResetStep: \"50\" [How often BFGS should reset history]\n",
      "<VERBOSE>                          :     Tau: \"3.000000e+00\" [LineSearch \"size step\"]\n",
      "<VERBOSE>                          :     BPMode: \"sequential\" [Back-propagation learning mode: sequential or batch]\n",
      "<VERBOSE>                          :     BatchSize: \"-1\" [Batch size: number of events/batch, only set if in Batch Mode, -1 for BatchSize=number_of_events]\n",
      "<VERBOSE>                          :     ConvergenceImprove: \"1.000000e-30\" [Minimum improvement which counts as improvement (<0 means automatic convergence check is turned off)]\n",
      "<VERBOSE>                          :     ConvergenceTests: \"-1\" [Number of steps (without improvement) required for convergence (<0 means automatic convergence check is turned off)]\n",
      "<VERBOSE>                          :     UseRegulator: \"False\" [Use regulator to avoid over-training]\n",
      "<VERBOSE>                          :     UpdateLimit: \"10000\" [Maximum times of regulator update]\n",
      "<VERBOSE>                          :     CalculateErrors: \"False\" [Calculates inverse Hessian matrix at the end of the training to be able to calculate the uncertainties of an MVA value]\n",
      "<VERBOSE>                          :     WeightRange: \"1.000000e+00\" [Take the events for the estimator calculations from small deviations from the desired value to large deviations only over the weight range]\n",
      "<HEADER> MLPfinal                 : [dataset] : Create Transformation \"Norm\" with events from all classes.\n",
      "                         : \n",
      "<HEADER>                          : Transformation, Variable selection : \n",
      "                         : Input : variable 'ADC_mean' <---> Output : variable 'ADC_mean'\n",
      "                         : Input : variable 'nhits_min' <---> Output : variable 'nhits_min'\n",
      "                         : Input : variable 'entry_dist' <---> Output : variable 'entry_dist'\n",
      "                         : Input : variable 'exit_dist' <---> Output : variable 'exit_dist'\n",
      "                         : Input : variable 'docasqrx_max' <---> Output : variable 'docasqrx_max'\n",
      "                         : Input : variable 'docasqry_max' <---> Output : variable 'docasqry_max'\n",
      "<HEADER> MLPfinal                 : Building Network. \n",
      "                         : Initializing weights\n"
     ]
    }
   ],
   "source": [
    "factory.BookMethod(dataLoader, TMVA.Types.kMLP, \"MLPfinal\",\"H:V:VarTransform=Norm:NCycles=300:HiddenLayers=N+10:TestRate=10:TrainingMethod=BP\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "784292be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HEADER> Factory                  : Train all methods\n",
      "                         : Rebuilding Dataset dataset\n",
      "                         : Building event vectors for type 0 Signal\n",
      "                         : Dataset[dataset] :  create input formulas for tree sig\n",
      "                         : Building event vectors for type 1 Signal\n",
      "                         : Dataset[dataset] :  create input formulas for tree sig\n",
      "                         : Building event vectors for type 0 Background\n",
      "                         : Dataset[dataset] :  create input formulas for tree bkg\n",
      "                         : Building event vectors for type 1 Background\n",
      "                         : Dataset[dataset] :  create input formulas for tree bkg\n",
      "<HEADER> DataSetFactory           : [dataset] : Number of events in input trees\n",
      "                         : \n",
      "                         : \n",
      "                         : Dataset[dataset] : Weight renormalisation mode: \"EqualNumEvents\": renormalises all event classes ...\n",
      "                         : Dataset[dataset] :  such that the effective (weighted) number of events in each class is the same \n",
      "                         : Dataset[dataset] :  (and equals the number of events (entries) given for class=0 )\n",
      "                         : Dataset[dataset] : ... i.e. such that Sum[i=1..N_j]{w_i} = N_classA, j=classA, classB, ...\n",
      "                         : Dataset[dataset] : ... (note that N_j is the sum of TRAINING events\n",
      "                         : Dataset[dataset] :  ..... Testing events are not renormalised nor included in the renormalisation factor!)\n",
      "                         : Number of training and testing events\n",
      "                         : ---------------------------------------------------------------------------\n",
      "                         : Signal     -- training events            : 104918\n",
      "                         : Signal     -- testing events             : 105370\n",
      "                         : Signal     -- training and testing events: 210288\n",
      "                         : Background -- training events            : 1565055\n",
      "                         : Background -- testing events             : 1563066\n",
      "                         : Background -- training and testing events: 3128121\n",
      "                         : \n",
      "<HEADER> DataSetInfo              : Correlation matrix (Signal):\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :               ADC_mean nhits_min entry_dist exit_dist docasqrx_max docasqry_max\n",
      "                         :     ADC_mean:   +1.000    +0.164     -0.120    -0.116       -0.053       -0.101\n",
      "                         :    nhits_min:   +0.164    +1.000     -0.146    -0.136       -0.034       -0.085\n",
      "                         :   entry_dist:   -0.120    -0.146     +1.000    +0.319       +0.124       +0.318\n",
      "                         :    exit_dist:   -0.116    -0.136     +0.319    +1.000       +0.118       +0.295\n",
      "                         : docasqrx_max:   -0.053    -0.034     +0.124    +0.118       +1.000       +0.118\n",
      "                         : docasqry_max:   -0.101    -0.085     +0.318    +0.295       +0.118       +1.000\n",
      "                         : -------------------------------------------------------------------------------\n",
      "<HEADER> DataSetInfo              : Correlation matrix (Background):\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :               ADC_mean nhits_min entry_dist exit_dist docasqrx_max docasqry_max\n",
      "                         :     ADC_mean:   +1.000    +0.603     -0.126    -0.347       +0.051       +0.104\n",
      "                         :    nhits_min:   +0.603    +1.000     -0.160    -0.309       +0.041       +0.095\n",
      "                         :   entry_dist:   -0.126    -0.160     +1.000    +0.077       +0.003       +0.005\n",
      "                         :    exit_dist:   -0.347    -0.309     +0.077    +1.000       -0.015       -0.039\n",
      "                         : docasqrx_max:   +0.051    +0.041     +0.003    -0.015       +1.000       +0.015\n",
      "                         : docasqry_max:   +0.104    +0.095     +0.005    -0.039       +0.015       +1.000\n",
      "                         : -------------------------------------------------------------------------------\n",
      "<HEADER> DataSetFactory           : [dataset] :  \n",
      "                         : \n",
      "<HEADER> Factory                  : [dataset] : Create Transformation \"I\" with events from all classes.\n",
      "                         : \n",
      "<HEADER>                          : Transformation, Variable selection : \n",
      "                         : Input : variable 'ADC_mean' <---> Output : variable 'ADC_mean'\n",
      "                         : Input : variable 'nhits_min' <---> Output : variable 'nhits_min'\n",
      "                         : Input : variable 'entry_dist' <---> Output : variable 'entry_dist'\n",
      "                         : Input : variable 'exit_dist' <---> Output : variable 'exit_dist'\n",
      "                         : Input : variable 'docasqrx_max' <---> Output : variable 'docasqrx_max'\n",
      "                         : Input : variable 'docasqry_max' <---> Output : variable 'docasqry_max'\n",
      "<HEADER> TFHandler_Factory        :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :     ADC_mean:        1849.9        1266.0   [        180.35        3711.6 ]\n",
      "                         :    nhits_min:        62.386        49.067   [        0.0000        713.00 ]\n",
      "                         :   entry_dist:        21.917        69.110   [    2.4981e-06        762.00 ]\n",
      "                         :    exit_dist:        73.586        144.43   [    7.6447e-05        761.91 ]\n",
      "                         : docasqrx_max:        92.256        921.42   [        0.0000    1.1447e+06 ]\n",
      "                         : docasqry_max:        84.501        411.04   [        0.0000    5.6439e+05 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : Ranking input variables (method unspecific)...\n",
      "<HEADER> IdTransformation         : Ranking result (top variable is best ranked)\n",
      "                         : -------------------------------------\n",
      "                         : Rank : Variable     : Separation\n",
      "                         : -------------------------------------\n",
      "                         :    1 : ADC_mean     : 8.285e-01\n",
      "                         :    2 : docasqry_max : 3.020e-01\n",
      "                         :    3 : exit_dist    : 2.346e-01\n",
      "                         :    4 : nhits_min    : 1.779e-01\n",
      "                         :    5 : docasqrx_max : 1.360e-01\n",
      "                         :    6 : entry_dist   : 5.036e-02\n",
      "                         : -------------------------------------\n",
      "<HEADER> Factory                  : Train method: MLPfinal for Classification\n",
      "                         : \n",
      "                         : \n",
      "                         : ================================================================\n",
      "                         : H e l p   f o r   M V A   m e t h o d   [ MLPfinal ] :\n",
      "                         : \n",
      "                         : --- Short description:\n",
      "                         : \n",
      "                         : The MLP artificial neural network (ANN) is a traditional feed-\n",
      "                         : forward multilayer perceptron implementation. The MLP has a user-\n",
      "                         : defined hidden layer architecture, while the number of input (output)\n",
      "                         : nodes is determined by the input variables (output classes, i.e., \n",
      "                         : signal and one background). \n",
      "                         : \n",
      "                         : --- Performance optimisation:\n",
      "                         : \n",
      "                         : Neural networks are stable and performing for a large variety of \n",
      "                         : linear and non-linear classification problems. However, in contrast\n",
      "                         : to (e.g.) boosted decision trees, the user is advised to reduce the \n",
      "                         : number of input variables that have only little discrimination power. \n",
      "                         : \n",
      "                         : In the tests we have carried out so far, the MLP and ROOT networks\n",
      "                         : (TMlpANN, interfaced via TMVA) performed equally well, with however\n",
      "                         : a clear speed advantage for the MLP. The Clermont-Ferrand neural \n",
      "                         : net (CFMlpANN) exhibited worse classification performance in these\n",
      "                         : tests, which is partly due to the slow convergence of its training\n",
      "                         : (at least 10k training cycles are required to achieve approximately\n",
      "                         : competitive results).\n",
      "                         : \n",
      "                         : Overtraining: only the TMlpANN performs an explicit separation of the\n",
      "                         : full training sample into independent training and validation samples.\n",
      "                         : We have found that in most high-energy physics applications the \n",
      "                         : available degrees of freedom (training events) are sufficient to \n",
      "                         : constrain the weights of the relatively simple architectures required\n",
      "                         : to achieve good performance. Hence no overtraining should occur, and \n",
      "                         : the use of validation samples would only reduce the available training\n",
      "                         : information. However, if the performance on the training sample is \n",
      "                         : found to be significantly better than the one found with the inde-\n",
      "                         : pendent test sample, caution is needed. The results for these samples \n",
      "                         : are printed to standard output at the end of each training job.\n",
      "                         : \n",
      "                         : --- Performance tuning via configuration options:\n",
      "                         : \n",
      "                         : The hidden layer architecture for all ANNs is defined by the option\n",
      "                         : \"HiddenLayers=N+1,N,...\", where here the first hidden layer has N+1\n",
      "                         : neurons and the second N neurons (and so on), and where N is the number  \n",
      "                         : of input variables. Excessive numbers of hidden layers should be avoided,\n",
      "                         : in favour of more neurons in the first hidden layer.\n",
      "                         : \n",
      "                         : The number of cycles should be above 500. As said, if the number of\n",
      "                         : adjustable weights is small compared to the training sample size,\n",
      "                         : using a large number of training samples should not lead to overtraining.\n",
      "                         : \n",
      "                         : <Suppress this message by specifying \"!H\" in the booking option>\n",
      "                         : ================================================================\n",
      "                         : \n",
      "<HEADER> TFHandler_MLPfinal       :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :     ADC_mean:     -0.054431       0.71704   [       -1.0000        1.0000 ]\n",
      "                         :    nhits_min:      -0.82500       0.13764   [       -1.0000        1.0000 ]\n",
      "                         :   entry_dist:      -0.94247       0.18139   [       -1.0000        1.0000 ]\n",
      "                         :    exit_dist:      -0.80684       0.37913   [       -1.0000        1.0000 ]\n",
      "                         : docasqrx_max:      -0.99984     0.0016099   [       -1.0000        1.0000 ]\n",
      "                         : docasqry_max:      -0.99970     0.0014566   [       -1.0000        1.0000 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : Training Network\n",
      "                         : \n",
      "                         : Elapsed time for training with 1669973 events: 2.61e+03 sec         \n",
      "<HEADER> MLPfinal                 : [dataset] : Evaluation of MLPfinal on training sample (1669973 events)\n",
      "                         : Elapsed time for evaluation of 1669973 events: 4.05 sec       \n",
      "                         : Creating xml weight file: dataset/weights/tmvaTest_MLPfinal.weights.xml\n",
      "                         : Creating standalone class: dataset/weights/tmvaTest_MLPfinal.class.C\n",
      "                         : Write special histos to file: TMVAMLP.root:/dataset/Method_MLP/MLPfinal\n",
      "<HEADER> Factory                  : Training finished\n",
      "                         : \n",
      "                         : Ranking input variables (method specific)...\n",
      "<HEADER> MLPfinal                 : Ranking result (top variable is best ranked)\n",
      "                         : -------------------------------------\n",
      "                         : Rank : Variable     : Importance\n",
      "                         : -------------------------------------\n",
      "                         :    1 : nhits_min    : 5.273e+02\n",
      "                         :    2 : entry_dist   : 4.610e+02\n",
      "                         :    3 : docasqrx_max : 2.471e+02\n",
      "                         :    4 : exit_dist    : 2.257e+02\n",
      "                         :    5 : docasqry_max : 1.816e+02\n",
      "                         :    6 : ADC_mean     : 1.730e+02\n",
      "                         : -------------------------------------\n",
      "<HEADER> Factory                  : === Destroy and recreate all methods via weight files for testing ===\n",
      "                         : \n",
      "                         : Reading weight file: dataset/weights/tmvaTest_MLPfinal.weights.xml\n",
      "<HEADER> MLPfinal                 : Building Network. \n",
      "                         : Initializing weights\n",
      "<HEADER> Factory                  : Test all methods\n",
      "<HEADER> Factory                  : Test method: MLPfinal for Classification performance\n",
      "                         : \n",
      "<HEADER> MLPfinal                 : [dataset] : Evaluation of MLPfinal on testing sample (1668436 events)\n",
      "                         : Elapsed time for evaluation of 1668436 events: 3.4 sec       \n",
      "<HEADER> Factory                  : Evaluate all methods\n",
      "<HEADER> Factory                  : Evaluate classifier: MLPfinal\n",
      "                         : \n",
      "<HEADER> TFHandler_MLPfinal       :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :     ADC_mean:     -0.053443       0.71598   [      -0.98359        1.0029 ]\n",
      "                         :    nhits_min:      -0.82461       0.13798   [       -1.0000        2.7924 ]\n",
      "                         :   entry_dist:      -0.94260       0.18149   [       -1.0000       0.99871 ]\n",
      "                         :    exit_dist:      -0.80781       0.37835   [       -1.0000       0.99958 ]\n",
      "                         : docasqrx_max:      -0.99984     0.0011545   [       -1.0000     0.0044364 ]\n",
      "                         : docasqry_max:      -0.99970     0.0017026   [       -1.0000        1.6792 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "<HEADER> MLPfinal                 : [dataset] : Loop over test events and fill histograms with classifier response...\n",
      "                         : \n",
      "<HEADER> TFHandler_MLPfinal       :     Variable            Mean            RMS    [        Min            Max ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         :     ADC_mean:     -0.053443       0.71598   [      -0.98359        1.0029 ]\n",
      "                         :    nhits_min:      -0.82461       0.13798   [       -1.0000        2.7924 ]\n",
      "                         :   entry_dist:      -0.94260       0.18149   [       -1.0000       0.99871 ]\n",
      "                         :    exit_dist:      -0.80781       0.37835   [       -1.0000       0.99958 ]\n",
      "                         : docasqrx_max:      -0.99984     0.0011545   [       -1.0000     0.0044364 ]\n",
      "                         : docasqry_max:      -0.99970     0.0017026   [       -1.0000        1.6792 ]\n",
      "                         : -------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Evaluation results ranked by best signal efficiency and purity (area)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet       MVA                       \n",
      "                         : Name:         Method:          ROC-integ\n",
      "                         : dataset       MLPfinal       : 0.991\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "                         : Testing efficiency compared to training efficiency (overtraining check)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) \n",
      "                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   \n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : dataset              MLPfinal       : 0.897 (0.897)       0.968 (0.968)      1.000 (1.000)\n",
      "                         : -------------------------------------------------------------------------------------------------------------------\n",
      "                         : \n",
      "<HEADER> Dataset:dataset          : Created tree 'TestTree' with 1668436 events\n",
      "                         : \n",
      "<HEADER> Dataset:dataset          : Created tree 'TrainTree' with 1669973 events\n",
      "                         : \n",
      "<HEADER> Factory                  : Thank you for using TMVA!\n",
      "                         : For citation information, please visit: http://tmva.sf.net/citeTMVA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%, time left: unknown\n",
      "6%, time left: 39 mins\n",
      "13%, time left: 33 mins\n",
      "19%, time left: 30 mins\n",
      "25%, time left: 26 mins\n",
      "31%, time left: 25 mins\n",
      "38%, time left: 24 mins\n",
      "44%, time left: 23 mins\n",
      "50%, time left: 20 mins\n",
      "56%, time left: 18 mins\n",
      "63%, time left: 15 mins\n",
      "69%, time left: 12 mins\n",
      "75%, time left: 10 mins\n",
      "81%, time left: 7 mins\n",
      "88%, time left: 5 mins\n",
      "94%, time left: 153 sec\n",
      "0%, time left: unknown\n",
      "6%, time left: 3 sec\n",
      "12%, time left: 3 sec\n",
      "18%, time left: 3 sec\n",
      "25%, time left: 2 sec\n",
      "31%, time left: 2 sec\n",
      "37%, time left: 2 sec\n",
      "43%, time left: 2 sec\n",
      "50%, time left: 1 sec\n",
      "56%, time left: 1 sec\n",
      "62%, time left: 1 sec\n",
      "68%, time left: 1 sec\n",
      "75%, time left: 0 sec\n",
      "81%, time left: 0 sec\n",
      "87%, time left: 0 sec\n",
      "93%, time left: 0 sec\n",
      "0%, time left: unknown\n",
      "6%, time left: 2 sec\n",
      "12%, time left: 2 sec\n",
      "18%, time left: 2 sec\n",
      "25%, time left: 2 sec\n",
      "31%, time left: 2 sec\n",
      "37%, time left: 1 sec\n",
      "43%, time left: 1 sec\n",
      "50%, time left: 1 sec\n",
      "56%, time left: 1 sec\n",
      "62%, time left: 1 sec\n",
      "68%, time left: 0 sec\n",
      "75%, time left: 0 sec\n",
      "81%, time left: 0 sec\n",
      "87%, time left: 0 sec\n",
      "93%, time left: 0 sec\n"
     ]
    }
   ],
   "source": [
    "#Train, Test and Evaluate all methods\n",
    "factory.TrainAllMethods()\n",
    "factory.TestAllMethods()\n",
    "factory.EvaluateAllMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c3318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote root file TMVA.root\n",
      "TMVA analysis is done\n"
     ]
    }
   ],
   "source": [
    "#Save output and finish up\n",
    "outputFile.Close()\n",
    "print(\"wrote root file TMVA.root\")\n",
    "print(\"TMVA analysis is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54547fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
